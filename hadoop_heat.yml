# ===========================================
# Copyright NeCTAR, May 2014, all rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License"); 
# you may not use this file except in compliance with the License. 
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, 
# software distributed under the License is distributed on an "AS IS" BASIS, 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions 
# and limitations under the License.
# ===========================================
# hadoop_heat.yml
# ===========================================
HeatTemplateFormatVersion: '2012-12-12'
Description: "An Hadoop Cluster. "
Parameters: 
    ProjectName: 
        Description: "The name of the Hadoop project. Used to name the configuration directory"
        Type: "String"
        Default: "unicarbkb"
    TemporaryInstallerPassword: 
        Description: "A password used to finalised configuration across cluster members. Randomise this before building the stack. "
        Type: "String"
        Default: "0^3Dfxx"
    KeyName: 
        Description: "Name of an existing EC2 KeyPair to enable SSH access to the instance. "
        Type: "String"
        Default: "richardr_on_nectar"
    SlaveCount:
        Type: "Number"
        Default: "2"
        MinValue: "1"
        MaxValue: "100"
        Description: "The number of slave instances to create"
    InstanceType:
        Description: "The size of the VM. "
        Type: String
        Default: "m1.small"
        AllowedValues:
            - m1.small
            - m1.medium
            - m1.large
            - m1.xlarge
            - m1.xxlarge
    ImageId:
        Description: "The base VM image used to build the cluster. "
        Type: String
        # Customised Ubuntu 12.04 image on NeCTAR
        # Snapshot called java_seed_08May2014.
        # Made by running sudo apt-get install python-setuptools
        # Add more image options later.
        Default: c64cacf2-c269-4a95-aae6-7f1a3913f9eb
        #Default: "ubuntu-12.04"
        #AllowedValues: ["ubuntu-12.04","ubuntu-12.4"]
    AvailabilityZone: 
        Description: "Physical location of the running cluster. "
        Type: String
        Default: "NCI"
        AllowedValues:
            - NCI
            - melbourne
            - melbourne-np
            - melbourne-qh2
            - qld
            - sa
    BaseDomain: 
        Description: "DynDNS base domain name. Used to synthesise cluster node domain names. "
        Type: String
        Default: "doesntexist.org"
    MasterNodeName: 
        Description: "Name for Hadoop master node, hosting JobTracker and NameNode. Used to construct domain name. "
        Type: String
        Default: "hadoop-master"
    AuxiliaryNodeName: 
        Description: "Name for Hadoop auxiliary node, hosting SecondaryNameNode. Used to construct domain name. "
        Type: String
        Default: "hadoop-auxiliary"
    SlaveNodeName: 
        Description: "Base name for Hadoop slave node, hosting TaskTracker and DataNode. Used to construct domain name. "
        Type: String
        Default: "hadoop-slave"
#Mappings:
#    InstanceId:
#        'ubuntu-12.04': {ImageId: c64cacf2-c269-4a95-aae6-7f1a3913f9eb}
Resources: 
    # http://docs.openstack.org/developer/heat/template_guide/cfn.html#AWS::EC2::SecurityGroup
    DefaultSecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
            GroupDescription: Standard firewall rules
            SecurityGroupIngress:
                - {IpProtocol: icmp, FromPort: '-1', ToPort: '-1', CidrIp : 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '22', ToPort: '22', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '80', ToPort: '80', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '443', ToPort: '443', CidrIp: 0.0.0.0/0}
    MemberSecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
            GroupDescription: Use a dummy security group as a tag signifying belonging to the cluster.
    ClusterSecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
            GroupDescription: Cluster commmunications allowed only from other cluster members.
            SecurityGroupIngress:
                - {IpProtocol: tcp, FromPort: '1', ToPort: '65535', SourceSecurityGroupName: {Ref: MemberSecurityGroup}}
    WebManagementSecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
            GroupDescription: Expose the Hadoop web-based management interfaces.
            SecurityGroupIngress:
                - {IpProtocol: tcp, FromPort: '50070', ToPort: '50070', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '50030', ToPort: '50030', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '50060', ToPort: '50060', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '50075', ToPort: '50075', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '50090', ToPort: '50090', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '60010', ToPort: '60010', CidrIp: 0.0.0.0/0}
                - {IpProtocol: tcp, FromPort: '50470', ToPort: '50740', CidrIp: 0.0.0.0/0}
    HadoopSlaveGroup:
        Type: "OS::Heat::InstanceGroup"
        Properties:
            # Availability zones here are both required and yet resolutely ignored.
            # Martin says.
            AvailabilityZones: [Ref: "AvailabilityZone"] # Fn::GetAZs: ""
            LaunchConfigurationName: {Ref: "HadoopSlaveConfig"}
            Size: {Ref: SlaveCount}
    HadoopSlaveConfig: 
        Type: "AWS::AutoScaling::LaunchConfiguration"
        Properties: 
            KeyName: {Ref: KeyName}
            #ImageId: {Fn::FindInMap: [InstanceId, {Ref: ImageName}, ImageId]}
            ImageId:  {Ref: ImageId}
            InstanceType: {Ref: InstanceType}
            SecurityGroups: [{Ref: "MemberSecurityGroup"}, {Ref: "ClusterSecurityGroup"}, {Ref: "WebManagementSecurityGroup"}, {Ref: "DefaultSecurityGroup"}]
            UserData:
                Fn::Base64:
                    Fn::Replace:
                      - _project_name_: {Ref: ProjectName}
                        _installer_account_password_: {Ref: TemporaryInstallerPassword}
                        _base_domain_: {Ref: BaseDomain}
                        _hadoop_master_name_: {Ref: MasterNodeName}
                        _hadoop_auxiliary_name_: {Ref: AuxiliaryNodeName}
                        _hadoop_slave_name_: {Ref: SlaveNodeName}
                        _hadoop_slave_count_: {Ref: SlaveCount}
                      - |
                        #!/bin/bash -v
                        apt-get update; apt-get upgrade
                        updatedb
                        cd /tmp
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_java.sh
                        chmod u+x install_java.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/bind_hadoop_directories.sh
                        chmod u+x bind_hadoop_directories.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_common.sh
                        chmod u+x install_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_common.sh
                        chmod u+x configure_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_hdfs.sh
                        chmod u+x configure_hdfs.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_slave_node.sh
                        chmod u+x install_slave_node.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_slave.sh
                        chmod u+x configure_slave.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_MRv1.sh
                        chmod u+x configure_MRv1.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/start_hdfs_slave.sh
                        chmod u+x start_hdfs_slave.sh
                        
                        export project_name=_project_name_
                        export installer_account_username="installer"
                        export installer_account_password=_installer_account_password_
                        
                        export hadoop_master_name=_hadoop_master_name_
                        export hadoop_master_domain=_hadoop_master_name_._base_domain_
                        
                        export hadoop_auxiliary_name=_hadoop_auxiliary_name_
                        export hadoop_auxiliary_domain=_hadoop_auxiliary_name_._base_domain_

                        export hadoop_slave_ip=`wget -qO -  http://169.254.169.254/latest/meta-data/local-ipv4`

                        export hadoop_node_ip=$hadoop_slave_ip
                        export hadoop_node_hostname=`hostname`
                       
                        ./install_common.sh
                        ./install_slave_node.sh
                        ./configure_common.sh
                        ./configure_hdfs.sh
                        ./configure_slave.sh
                        ./configure_MRv1.sh
                        
                        useradd -m -s /bin/bash $installer_account_username
                        echo "$installer_account_username:$installer_account_password" | chpasswd
                        inoticoming /home/installer --prefix host bash -c 'cat /home/installer/host_list >> /etc/hosts' \;
                        inoticoming /home/installer --prefix slaves mv -f /home/installer/slaves /etc/hadoop/conf._project_name_ \;
                        inoticoming /home/installer --prefix zoo.cfg mv -f /home/installer/zoo.cfg /etc/zookeeper/conf.dist/zoo.cfg \;
                        inoticoming /home/installer --prefix finaliser /tmp/start_hdfs_slave.sh \;
                        inoticoming /home/installer --prefix mapred_start service hadoop-0.20-mapreduce-tasktracker start \;
                        # Find this in /tmp
                        touch "installation_finished"
    HadoopSlave: 
        Type: "AWS::EC2::Instance"
        Properties: 
            KeyName: {Ref: KeyName}
            #ImageId: {Fn::FindInMap: [InstanceId, {Ref: ImageName}, ImageId]}
            ImageId:  {Ref: ImageId}
            InstanceType: {Ref: InstanceType}
            AvailabilityZone: {Ref: AvailabilityZone}
            SecurityGroups: 
                - {Ref: "MemberSecurityGroup"}
                - {Ref: "ClusterSecurityGroup"}
                - {Ref: "WebManagementSecurityGroup"}
                - {Ref: "DefaultSecurityGroup"}
            UserData:
                Fn::Base64:
                    Fn::Replace:
                      - _project_name_: {Ref: ProjectName}
                        _installer_account_password_: {Ref: TemporaryInstallerPassword}
                        _base_domain_: {Ref: BaseDomain}
                        _hadoop_master_name_: {Ref: MasterNodeName}
                        _hadoop_auxiliary_name_: {Ref: AuxiliaryNodeName}
                        _hadoop_slave_name_: {Ref: SlaveNodeName}
                      - |
                        #!/bin/bash -v
                        apt-get update; apt-get upgrade
                        updatedb
                        cd /tmp
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_java.sh
                        chmod u+x install_java.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/bind_hadoop_directories.sh
                        chmod u+x bind_hadoop_directories.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_common.sh
                        chmod u+x install_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_common.sh
                        chmod u+x configure_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_hdfs.sh
                        chmod u+x configure_hdfs.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_slave_node.sh
                        chmod u+x install_slave_node.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_slave.sh
                        chmod u+x configure_slave.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_MRv1.sh
                        chmod u+x configure_MRv1.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/start_hdfs_slave.sh
                        chmod u+x start_hdfs_slave.sh
                        
                        export project_name=_project_name_
                        export installer_account_username="installer"
                        export installer_account_password=_installer_account_password_
                        
                        export hadoop_master_name=_hadoop_master_name_
                        export hadoop_master_domain=_hadoop_master_name_._base_domain_
                        
                        export hadoop_auxiliary_name=_hadoop_auxiliary_name_
                        export hadoop_auxiliary_domain=_hadoop_auxiliary_name_._base_domain_

                        export hadoop_slave_ip=`wget -qO -  http://169.254.169.254/latest/meta-data/local-ipv4`

                        export hadoop_node_ip=$hadoop_slave_ip
                        export hadoop_node_hostname=`hostname`
                       
                        ./install_common.sh
                        ./install_slave_node.sh
                        ./configure_common.sh
                        ./configure_hdfs.sh
                        ./configure_slave.sh
                        ./configure_MRv1.sh
                        
                        useradd -m -s /bin/bash $installer_account_username
                        echo "$installer_account_username:$installer_account_password" | chpasswd
                        inoticoming /home/installer --prefix host bash -c 'cat /home/installer/host_list >> /etc/hosts' \;
                        inoticoming /home/installer --prefix slaves mv -f /home/installer/slaves /etc/hadoop/conf._project_name_ \;
                        inoticoming /home/installer --prefix zoo.cfg mv -f /home/installer/zoo.cfg /etc/zookeeper/conf.dist/zoo.cfg \;
                        inoticoming /home/installer --prefix finaliser /tmp/start_hdfs_slave.sh \;
                        inoticoming /home/installer --prefix mapred_start service hadoop-0.20-mapreduce-tasktracker start \;
                        # Find this in /tmp
                        touch "installation_finished"
                        
    HadoopAuxiliary: 
        Type: "AWS::EC2::Instance"
        Properties: 
            KeyName: {Ref: KeyName}
            #ImageId: {Fn::FindInMap: [InstanceId, {Ref: ImageName}, ImageId]}
            ImageId:  {Ref: ImageId}
            InstanceType: {Ref: InstanceType}
            AvailabilityZone: {Ref: AvailabilityZone}
            SecurityGroups: 
                - {Ref: "MemberSecurityGroup"}
                - {Ref: "ClusterSecurityGroup"}
                - {Ref: "WebManagementSecurityGroup"}
                - {Ref: "DefaultSecurityGroup"}
            UserData:
                Fn::Base64:
                    Fn::Replace:
                      - _project_name_: {Ref: ProjectName}
                        _installer_account_password_: {Ref: TemporaryInstallerPassword}
                        _base_domain_: {Ref: BaseDomain}
                        _hadoop_master_name_: {Ref: MasterNodeName}
                        _hadoop_auxiliary_name_: {Ref: AuxiliaryNodeName}
                      - |
                        #!/bin/bash -v
                        apt-get update; apt-get upgrade
                        updatedb
                        cd /tmp
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_java.sh
                        chmod u+x install_java.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/bind_hadoop_directories.sh
                        chmod u+x bind_hadoop_directories.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_common.sh
                        chmod u+x install_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_common.sh
                        chmod u+x configure_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_hdfs.sh
                        chmod u+x configure_hdfs.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_auxiliary_node.sh
                        chmod u+x install_auxiliary_node.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/start_hdfs_auxiliary.sh
                        chmod u+x start_hdfs_auxiliary.sh
                        
                        export project_name=_project_name_
                        export installer_account_username="installer"
                        export installer_account_password=_installer_account_password_
                        
                        export hadoop_master_name=_hadoop_master_name_
                        export hadoop_master_domain=_hadoop_master_name_._base_domain_

                        export hadoop_auxiliary_name=_hadoop_auxiliary_name_
                        export hadoop_auxiliary_domain=_hadoop_auxiliary_name_._base_domain_

                        export hadoop_auxiliary_ip=`wget -qO -  http://169.254.169.254/latest/meta-data/local-ipv4`

                        export hadoop_node_ip=$hadoop_auxiliary_ip
                        export hadoop_node_hostname=`hostname`
                                             
                        ./install_common.sh
                        ./install_auxiliary_node.sh
                        ./configure_common.sh
                        ./configure_hdfs.sh
                        
                        useradd -m -s /bin/bash $installer_account_username
                        echo "$installer_account_username:$installer_account_password" | chpasswd
                        inoticoming /home/installer --prefix host bash -c 'cat /home/installer/host_list >> /etc/hosts' \;
                        inoticoming /home/installer --prefix slaves mv -f /home/installer/slaves /etc/hadoop/conf._project_name_ \;
                        inoticoming /home/installer --prefix zoo.cfg mv -f /home/installer/zoo.cfg /etc/zookeeper/conf.dist/zoo.cfg  \;
                        inoticoming /home/installer --prefix finaliser /tmp/start_hdfs_auxiliary.sh \;
                        # Find this in /tmp
                        touch "installation_finished"
                        
                        
# http://docs.openstack.org/developer/heat/template_guide/cfn.html#AWS::EC2::Instance
# To get the node IP address see: https://wiki.openstack.org/wiki/Heat/ApplicationDeployment
    HadoopMaster: 
        Type: "AWS::EC2::Instance"
        Properties: 
            KeyName: {Ref: KeyName}
            #ImageId: {Fn::FindInMap: [InstanceId, {Ref: ImageName}, ImageId]}
            ImageId:  {Ref: ImageId}
            InstanceType: {Ref: InstanceType}
            AvailabilityZone: {Ref: AvailabilityZone}
            SecurityGroups: 
                - {Ref: "MemberSecurityGroup"}
                - {Ref: "ClusterSecurityGroup"}
                - {Ref: "WebManagementSecurityGroup"}
                - {Ref: "DefaultSecurityGroup"}
            UserData:
                Fn::Base64:
                    Fn::Replace:
                      - _project_name_: {Ref: ProjectName}
                        _installer_account_password_: {Ref: TemporaryInstallerPassword}
                        _base_domain_: {Ref: BaseDomain}
                        _hadoop_master_name_: {Ref: MasterNodeName}
                        _hadoop_auxiliary_name_: {Ref: AuxiliaryNodeName}
                        _hadoop_slave_ip_:
                            Fn::GetAtt: 
                                - "HadoopSlave"
                                - "PublicIp"
                        _hadoop_slave_hostname_:
                            Fn::GetAtt: 
                                - "HadoopSlave"
                                - "PublicDnsName"
                        _hadoop_auxiliary_ip_:
                            Fn::GetAtt: 
                                - "HadoopAuxiliary"
                                - "PublicIp"
                        _hadoop_auxiliary_hostname_:
                            Fn::GetAtt: 
                                - "HadoopAuxiliary"
                                - "PublicDnsName"
                        _hadoop_slave_name_: {Ref: SlaveNodeName}
                        _hadoop_slave_list_:
                            Fn::GetAtt: 
                                - "HadoopSlaveGroup"
                                - "InstanceList"
                        _hadoop_slave_count_: {Ref: SlaveCount}
                      - |
                        #!/bin/bash -v
                        apt-get update; apt-get upgrade
                        updatedb
                        #apt-get install -y git
                        cd /tmp
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_java.sh
                        chmod u+x install_java.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/bind_hadoop_directories.sh
                        chmod u+x bind_hadoop_directories.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_common.sh
                        chmod u+x install_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_common.sh
                        chmod u+x configure_common.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_hdfs.sh
                        chmod u+x configure_hdfs.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/start_hdfs_master.sh
                        chmod u+x start_hdfs_master.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/install_master_node.sh
                        chmod u+x install_master_node.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_master.sh
                        chmod u+x configure_master.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/configure_MRv1.sh
                        chmod u+x configure_MRv1.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/finalise_hadoop_installation.sh
                        chmod u+x finalise_hadoop_installation.sh
                        wget https://raw.githubusercontent.com/rrothwell/heat_hadoop/master/prep_map_reduce.sh
                        chmod u+x prep_map_reduce.sh
                        
                        export project_name=_project_name_
                        export installer_account_username="installer"
                        export installer_account_password=_installer_account_password_
                                                                       
                        export hadoop_master_name=_hadoop_master_name_
                        export hadoop_master_domain=_hadoop_master_name_._base_domain_
                        
                        export hadoop_auxiliary_name=_hadoop_auxiliary_name_
                        export hadoop_auxiliary_domain=_hadoop_auxiliary_name_._base_domain_
                        
                        export hadoop_slave_name_0=_hadoop_slave_name_
                        export hadoop_slave_domain_0=_hadoop_slave_name_._base_domain_
                        
                        export hadoop_base_domain=_base_domain_
                        export hadoop_master_ip=`wget -qO -  http://169.254.169.254/latest/meta-data/local-ipv4`
                       
                        export hadoop_slave_name=_hadoop_slave_name_
                        export hadoop_slave_list=_hadoop_slave_list_
                        export hadoop_slave_count=_hadoop_slave_count_
                        
                        export hadoop_auxiliary_hostname=_hadoop_auxiliary_hostname_
                        export hadoop_auxiliary_ip=_hadoop_auxiliary_ip_
                        
                        export hadoop_slave_hostname=_hadoop_slave_hostname_
                        export hadoop_slave_ip=_hadoop_slave_ip_
                        
                        export hadoop_node_ip=$hadoop_master_ip
                        export hadoop_node_hostname=`hostname`
                     
                        ./install_common.sh
                        ./install_master_node.sh
                        ./configure_common.sh
                        ./configure_hdfs.sh
                        ./configure_master.sh
                        ./configure_MRv1.sh
                        ./finalise_hadoop_installation.sh
                        ./start_hdfs_master.sh
                        ./prep_map_reduce.sh

Outputs:
    MasterNodeIp:
        Description: "Login via ssh using a command along these lines: "
        Value:
            Fn::Join:
                - ""
                -
                    - "ssh -i /path/to/private/key/file.pem root@"
                    -
                        Fn::GetAtt:
                            - "HadoopMaster"
                            - "PublicIp"
    AuxiliaryNodeIp:
        Description: "Login via ssh using this command: "
        Value:
            Fn::Join:
                - ""
                -
                    - "ssh -i /path/to/private/key/file.pem root@"
                    -
                        Fn::GetAtt:
                            - "HadoopAuxiliary"
                            - "PublicIp"
##    DynDNSMasterNodeDomainName:
        Description: "Enter this value for the domain name of the master node into the DynDNS account: "
        Value:
            Fn::Join:
                - ""
                -
                    - {Ref: MasterNodeName}
                    - "."
                    - {Ref: BaseDomain}
                    - ": "
                    -
                        Fn::GetAtt:
                            - "HadoopMaster"
                            - "PublicIp"
    DynDNSAuxilaryNodeDomainName:
        Description: "Enter this value for the domain name of the auxiliary node into the DynDNS account: "
        Value:
            Fn::Join:
                - ""
                -
                    - {Ref: AuxiliaryNodeName}
                    - "."
                    - {Ref: BaseDomain}
                    - ": "
                    -
                        Fn::GetAtt:
                            - "HadoopAuxiliary"
                            - "PublicIp"
